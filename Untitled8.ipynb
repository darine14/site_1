{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eb4C6jot6z5S",
        "outputId": "0b5fc3c6-c25a-4455-ec15-f46fbf8b3f56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scrapy\n",
            "  Downloading Scrapy-2.11.1-py2.py3-none-any.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.8/287.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Twisted>=18.9.0 (from scrapy)\n",
            "  Downloading twisted-23.10.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (42.0.2)\n",
            "Collecting cssselect>=0.9.1 (from scrapy)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy)\n",
            "  Downloading itemloaders-1.1.0-py3-none-any.whl (11 kB)\n",
            "Collecting parsel>=1.5.0 (from scrapy)\n",
            "  Downloading parsel-1.8.1-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.0.0)\n",
            "Collecting queuelib>=1.4.2 (from scrapy)\n",
            "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy)\n",
            "  Downloading service_identity-24.1.0-py3-none-any.whl (12 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading w3lib-2.1.2-py3-none-any.whl (21 kB)\n",
            "Collecting zope.interface>=5.1.0 (from scrapy)\n",
            "  Downloading zope.interface-6.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.3/247.3 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protego>=0.1.15 (from scrapy)\n",
            "  Downloading Protego-0.3.0-py2.py3-none-any.whl (8.5 kB)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy)\n",
            "  Downloading itemadapter-0.8.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from scrapy) (67.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.2)\n",
            "Collecting tldextract (from scrapy)\n",
            "  Downloading tldextract-5.1.1-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.7/97.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (4.9.4)\n",
            "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (23.2.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.5.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.3.0)\n",
            "Collecting automat>=0.8.0 (from Twisted>=18.9.0->scrapy)\n",
            "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting constantly>=15.1 (from Twisted>=18.9.0->scrapy)\n",
            "  Downloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
            "Collecting hyperlink>=17.1.1 (from Twisted>=18.9.0->scrapy)\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting incremental>=22.10.0 (from Twisted>=18.9.0->scrapy)\n",
            "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (4.9.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.6)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (2.31.0)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
            "  Downloading requests_file-2.0.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from automat>=0.8.0->Twisted>=18.9.0->scrapy) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2024.2.2)\n",
            "Installing collected packages: PyDispatcher, incremental, zope.interface, w3lib, queuelib, protego, jmespath, itemadapter, hyperlink, cssselect, constantly, automat, Twisted, requests-file, parsel, tldextract, service-identity, itemloaders, scrapy\n",
            "Successfully installed PyDispatcher-2.0.7 Twisted-23.10.0 automat-22.10.0 constantly-23.10.4 cssselect-1.2.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.8.0 itemloaders-1.1.0 jmespath-1.0.1 parsel-1.8.1 protego-0.3.0 queuelib-1.6.2 requests-file-2.0.0 scrapy-2.11.1 service-identity-24.1.0 tldextract-5.1.1 w3lib-2.1.2 zope.interface-6.2\n"
          ]
        }
      ],
      "source": [
        "!pip install scrapy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy startproject alg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnwmfpn47B-C",
        "outputId": "5951c228-e8ac-4f21-c653-7a2b75f2d5dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Scrapy project 'alg', using template directory '/usr/local/lib/python3.10/dist-packages/scrapy/templates/project', created in:\n",
            "    /content/alg\n",
            "\n",
            "You can start your first spider with:\n",
            "    cd alg\n",
            "    scrapy genspider example example.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/alg/alg/spiders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHThjNQs7InQ",
        "outputId": "a9e09dae-ea40-4dbe-ce8d-f8b9b9d070cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/alg/alg/spiders\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy genspider algnews https://www.algerie360.com/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEL9bGSU7POV",
        "outputId": "030dccfb-7567-4d0b-f6ef-366d8ce7cb95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created spider 'algnews' using template 'basic' in module:\n",
            "  alg.spiders.algnews\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile algnews.py\n",
        "\n",
        "import scrapy\n",
        "\n",
        "\n",
        "class AlgnewsSpider(scrapy.Spider):\n",
        "    name = \"algnews\"\n",
        "    allowed_domains = [\"www.algerie360.com\"]\n",
        "    start_urls = [\"https://www.algerie360.com/\"]\n",
        "\n",
        "    def parse(self, response):\n",
        "        news=response.css('article')\n",
        "        for new in news :\n",
        "          yield{\n",
        "                'titel' : new.css('h2 a::text').get(),\n",
        "                'url' : new.css('h2 a').attrib['href'],\n",
        "                'description' :new.css('p::text').get(),\n",
        "                  }\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y396a0wJ7W4t",
        "outputId": "7d7b9e1a-a443-45bf-8cea-3419d7419813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting algnews.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy crawl algnews -o data.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcw7jD5W7mL2",
        "outputId": "33b569af-2280-46dc-a19b-d959526c38f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-02-20 20:45:11 [scrapy.utils.log] INFO: Scrapy 2.11.1 started (bot: alg)\n",
            "2024-02-20 20:45:11 [scrapy.utils.log] INFO: Versions: lxml 4.9.4.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 23.10.0, Python 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0], pyOpenSSL 24.0.0 (OpenSSL 3.2.1 30 Jan 2024), cryptography 42.0.2, Platform Linux-6.1.58+-x86_64-with-glibc2.35\n",
            "2024-02-20 20:45:11 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2024-02-20 20:45:11 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2024-02-20 20:45:11 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2024-02-20 20:45:11 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2024-02-20 20:45:11 [scrapy.extensions.telnet] INFO: Telnet Password: c987ce19733bb014\n",
            "2024-02-20 20:45:11 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2024-02-20 20:45:11 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'alg',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'alg.spiders',\n",
            " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['alg.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2024-02-20 20:45:12 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2024-02-20 20:45:12 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2024-02-20 20:45:12 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2024-02-20 20:45:12 [scrapy.core.engine] INFO: Spider opened\n",
            "2024-02-20 20:45:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2024-02-20 20:45:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2024-02-20 20:45:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.algerie360.com/robots.txt> (referer: None)\n",
            "2024-02-20 20:45:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.algerie360.com/> (referer: None)\n",
            "2024-02-20 20:45:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.algerie360.com/>\n",
            "{'titel': 'Le père de Zidane explique pourquoi il est resté en France après l’indépendance ', 'url': 'https://www.algerie360.com/le-pere-de-zidane-explique-pourquoi-il-est-reste-en-france-apres-lindependance/', 'description': 'La vidéo de l’interview avec Ismaïl Zidane, père de Zinedine Zidane, date de six mois, certes, mais elle a été repostée par « RMC Sport » aujourd’hui sur les réseaux sociaux. Une…'}\n",
            "2024-02-20 20:45:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.algerie360.com/>\n",
            "{'titel': 'José Peseiro a décliné l’offre de la FAF ', 'url': 'https://www.algerie360.com/jose-peseiro-a-decline-loffre-de-la-faf/', 'description': 'José Peseiro n’entrainera pas l’équipe d’Algérie. Ce n’est finalement pas la fédération algérienne qui a écarté cette piste, mais c’est plutôt le technicien portugais qui a décliné l’offre. Pressenti pour succéder à…'}\n",
            "2024-02-20 20:45:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.algerie360.com/>\n",
            "{'titel': '1er jour du Ramadan 2024 : un institut astronomique égyptien fixe la date ', 'url': 'https://www.algerie360.com/1er-jour-ramadan-2024-un-institut-astronomique-egyptien-fixe-une-date/', 'description': \"Le président de l'Institut égyptien de recherche astronomique et géophysique a confirmé que le premier jour de Sha'ban (شعبان) 1445 H. sera astronomiquement le dimanche 11/02/2024, selon les calculs astronomiques effectués. Avec…\"}\n",
            "2024-02-20 20:45:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.algerie360.com/>\n",
            "{'titel': 'Exportations de l’Algérie hors hydrocarbures en 2023 : Zitouni dévoile la valeur ', 'url': 'https://www.algerie360.com/exportations-de-lalgerie-hors-hydrocarbures-en-2023-zitouni-devoile-la-valeur/', 'description': \"Le ministre du Commerce et de la Promotion des Exportations, Tayeb Zitouni, annonce un excédent commercial entre janvier et novembre 2023, atteignant 10,42 milliards de dollars américains. En effet, l'excédent commercial enregistré…\"}\n",
            "2024-02-20 20:45:12 [scrapy.core.scraper] DEBUG: Scraped from <200 https://www.algerie360.com/>\n",
            "{'titel': '«\\xa0La dignité du citoyen n’est pas un simple slogan\\xa0», affirme Tebboune ', 'url': 'https://www.algerie360.com/la-dignite-du-citoyen-nest-pas-un-simple-slogan-affirme-tebboune/', 'description': \"Le Président Abdelmadjid Tebboune a réaffirmé mardi dernier l'importance cruciale de tout effort visant à construire une Algérie nouvelle, fondée sur des bases solides qui répondent aux aspirations du peuple. Dans son…\"}\n",
            "2024-02-20 20:45:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.algerie360.com/> (referer: None)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/utils/defer.py\", line 279, in iter_errback\n",
            "    yield next(it)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/utils/python.py\", line 350, in __next__\n",
            "    return next(self.data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/utils/python.py\", line 350, in __next__\n",
            "    return next(self.data)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
            "    for r in iterable:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/spidermiddlewares/offsite.py\", line 28, in <genexpr>\n",
            "    return (r for r in result or () if self._filter(r, spider))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
            "    for r in iterable:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/spidermiddlewares/referer.py\", line 352, in <genexpr>\n",
            "    return (self._set_referer(r, response) for r in result or ())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
            "    for r in iterable:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/spidermiddlewares/urllength.py\", line 27, in <genexpr>\n",
            "    return (r for r in result or () if self._filter(r, spider))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
            "    for r in iterable:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/spidermiddlewares/depth.py\", line 31, in <genexpr>\n",
            "    return (r for r in result or () if self._filter(r, response, spider))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/scrapy/core/spidermw.py\", line 106, in process_sync\n",
            "    for r in iterable:\n",
            "  File \"/content/alg/alg/spiders/algnews.py\", line 15, in parse\n",
            "    'url' : new.css('h2 a').attrib['href'],\n",
            "KeyError: 'href'\n",
            "2024-02-20 20:45:12 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2024-02-20 20:45:12 [scrapy.extensions.feedexport] INFO: Stored json feed (5 items) in: data.json\n",
            "2024-02-20 20:45:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 448,\n",
            " 'downloader/request_count': 2,\n",
            " 'downloader/request_method_count/GET': 2,\n",
            " 'downloader/response_bytes': 67052,\n",
            " 'downloader/response_count': 2,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'elapsed_time_seconds': 0.482872,\n",
            " 'feedexport/success_count/FileFeedStorage': 1,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2024, 2, 20, 20, 45, 12, 565915, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 407762,\n",
            " 'httpcompression/response_count': 2,\n",
            " 'item_scraped_count': 5,\n",
            " 'log_count/DEBUG': 10,\n",
            " 'log_count/ERROR': 1,\n",
            " 'log_count/INFO': 11,\n",
            " 'memusage/max': 162291712,\n",
            " 'memusage/startup': 162291712,\n",
            " 'response_received_count': 2,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'spider_exceptions/KeyError': 1,\n",
            " 'start_time': datetime.datetime(2024, 2, 20, 20, 45, 12, 83043, tzinfo=datetime.timezone.utc)}\n",
            "2024-02-20 20:45:12 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    }
  ]
}